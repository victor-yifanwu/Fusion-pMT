{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab03b16",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-01T06:34:14.850939Z",
     "iopub.status.busy": "2024-02-01T06:34:14.850600Z",
     "iopub.status.idle": "2024-02-01T06:34:20.378434Z",
     "shell.execute_reply": "2024-02-01T06:34:20.377414Z"
    },
    "papermill": {
     "duration": 5.534057,
     "end_time": "2024-02-01T06:34:20.380808",
     "exception": false,
     "start_time": "2024-02-01T06:34:14.846751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417e1527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T06:34:20.387072Z",
     "iopub.status.busy": "2024-02-01T06:34:20.386573Z",
     "iopub.status.idle": "2024-02-01T06:34:20.732696Z",
     "shell.execute_reply": "2024-02-01T06:34:20.731467Z"
    },
    "papermill": {
     "duration": 0.351562,
     "end_time": "2024-02-01T06:34:20.734833",
     "exception": false,
     "start_time": "2024-02-01T06:34:20.383271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (embedding): Embedding(23, 256)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, device):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.positional_encoding = self._generate_positional_encoding(d_model, max_seq_length).to(device)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def _generate_positional_encoding(self, dim, max_len):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) + self.positional_encoding[:src.size(0), :]\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "vocab_size = 23  # 21种氨基酸 + 2个用于填充\n",
    "d_model = 256  # 嵌入维度\n",
    "nhead = 4  # 注意力机制的头数 #考虑调小\n",
    "num_encoder_layers = 2  # 编码器层的数量\n",
    "dim_feedforward = 128  # 前馈网络的维度 #调小\n",
    "max_seq_length = 22  # 序列的最大长度 #考虑优化\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_seq_length, device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93153a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T06:34:20.740634Z",
     "iopub.status.busy": "2024-02-01T06:34:20.740326Z",
     "iopub.status.idle": "2024-02-01T06:34:28.187030Z",
     "shell.execute_reply": "2024-02-01T06:34:28.185980Z"
    },
    "papermill": {
     "duration": 7.452471,
     "end_time": "2024-02-01T06:34:28.189570",
     "exception": false,
     "start_time": "2024-02-01T06:34:20.737099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1207356452941895\n",
      "Epoch 11, Loss: 0.0248120054602623\n",
      "Epoch 21, Loss: 0.00518172699958086\n",
      "Epoch 31, Loss: 0.0028125355020165443\n",
      "Epoch 41, Loss: 0.0020934364292770624\n",
      "Epoch 51, Loss: 0.0018653686856850982\n",
      "Epoch 61, Loss: 0.002030816162005067\n",
      "Epoch 71, Loss: 0.001400624169036746\n",
      "Epoch 81, Loss: 0.0012842409778386354\n",
      "Epoch 91, Loss: 0.001004404854029417\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = pd.read_csv('/kaggle/input/pmtnetdata/data/tcr_seq.csv')\n",
    "sequences = data['Amino.Acid'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(\"ACDEFGHIKLMNPQRSTVWYOX\"))  # 所有可能的氨基酸\n",
    "encoded_sequences = []\n",
    "for sequence in sequences:\n",
    "    encoded_sequence = label_encoder.transform(list(sequence))\n",
    "    encoded_sequences.append(encoded_sequence)\n",
    "\n",
    "max_seq_length = max([len(seq) for seq in encoded_sequences])\n",
    "padded_sequences = np.zeros((len(encoded_sequences), max_seq_length), dtype=int)\n",
    "for i, seq in enumerate(encoded_sequences):\n",
    "    padded_sequences[i, :len(seq)] = seq[:max_seq_length]\n",
    "\n",
    "X_train, X_val, _, _ = train_test_split(padded_sequences, np.zeros(len(padded_sequences)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long).to(device))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.long).to(device))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), inputs.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4348181,
     "sourceId": 7469532,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 161156665,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.849821,
   "end_time": "2024-02-01T06:34:29.916331",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-01T06:34:12.066510",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
